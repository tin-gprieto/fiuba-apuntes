- **[DAH] — sec. 7.1**: _Scheduling_
- **[ARP] — cap. 7**: _Scheduling: Introduction_
- **[ARP] — cap. 8**: _Scheduling: The Multi-Level Feedback Queue_
- **[ARP] — cap. 10**: _Multiprocessor Scheduling_
    
- Opcional: **[IA-3A] — cap. 6**: _Interrupt and Exception Handling_
- [7 - Scheduling: Introduction](https://pages.cs.wisc.edu/~remzi/OSTEP/cpu-sched.pdf)
- [9 - Scheduling: Proportional Share](https://pages.cs.wisc.edu/~remzi/OSTEP/cpu-sched-lottery.pdf)

-------------------------------------

_time slice / time quantum_ : tiempo que el kernel le otorga a un proceso dado para que sea ejecutado por el CPU,

## Time Sharing

Idea: tener toda una computadora para uno mismo -> Introduce la *multi-programación*

## Multi-programación 

Contexto: En caso de que un programa deba realizar una tarea I/O, debe interactuar con un dispositivo I/O. 

Problema: Demasiado tiempo de ejecución en instrucciones de I/O (80% /90% de tiempo total del programa). El CPU permanece inactivo mientras se realizan estas operaciones.

Solución: Utilizar la CPU con otros procesos listos a ser computados mientras el proceso original realiza tareas de I/O. Para esto, la computadora debe poder cargar varios procesos en memoria.

#### Utilización del CPU

Si se asume que el *20%* de tiempo de ejecución de un programa es _computo_ y el *80%* son operaciones I/O. Entonces, 5 procesos, ocuparían el total de la memoria.

! - Operaciones I/O son *bloqueantes* 

*Formula de utilización del CPU :*
$$
1 - p^n
$$
$$ p : \text{fracción de tiempo bloqueando una operación I/O}$$
$$n : \text{cantidad de procesos}$$
$$p^{n} : \text{resulta ser la probabilidad de que n procesos esten realizando tareas I/O}$$
Ejemplos: 
- 1 proceso con el 80% con tareas de I/O -> 1 - 0,8 = 0,2 -> 20% de ocupación del CPU
- 3 procesos con || -> 1 - 0,8^3 = 0,488 -> 48,8% de ocupación del CPU

#### Multiple fixed partitions

En el contexto de cargar procesos en memoria, una posibilidad era realizar varias particiones de tamaño fijo. Es así como la multi programación ocurre entre las particiones y no existe memoria compartida. 

![[sisiop-mfp.png]]
#### Multiple variable partitions

En este caso, se trata a toda la memoria como un único espacio donde se puede asignar "regiones". Cada proceso puede tener cualquier tamaño de memoria asignada.

![[sisop-mvp.png]]

La entidad que se encarga de coordinar la forma en la que se ejecuta la multi programación de varios procesos es el **Planificador o scheduler**.

## Números y Workload

**Workload**: Carga de trabajo de un proceso corriendo en el sistema. Es fundamental para determinar partes de las políticas de planificación.

Contiene supuestos irreales como: 
- Todos los procesos se ejecutan en el CPU
- El tiempo de ejecución (_runtime_) de cada proceso es conocido

### Métricas de planificación 

Para comparar diferentes políticas de planificación se necesita algún tipo de métrica estandarizada para medir performance:

_Turn around time:_ T_around = T_completion - T_arrival
* T_completion es absoluto al inicio de ejecución del primer proceso

_Time to response_: T_response = T_firstrun - T_arrival
- T_firstrun: Tiempo en el que el proceso se ejecuta por primera vez en el CPU
-Ejemplo: 


|  A  |  B  |  C  |      A      |
0     10    20    30           ...
      |
      Ta de B y C

Tr A = 0s, Tr B = 10 - 10 = 0s, Tr C = 20 - 10 = 10s
 

## Políticas para sistemas Mono-procesador

### FIFO (First in - first out)

Los procesos se ejecutan en orden de llegada y ocupan el CPU hasta que termine su ejecución. 

Supuestos: los procesos A,B,C con T_arrival=0s

A=B=C=10s -> Taround = 10+20+30/3 = 20s

|  A   |   B   |   C   |
0      10      20      30

A=100s, B=C=10s -> Taround = 100+110+120/3 = 110s

|              A            |  B  |  C  |
0                          100   110    120

*Convoy effect*: Cuando un proceso que consume mucho tiempo de ejecución opaca al procesos mas pequeños.

### Shortest Job First (SJF)

Para resolver este problema se puede implementar que se ejecute primero el proceso de duración minima. 


|  B  |  C  |              A            |
0     10    20                         120

Taround = 10+20+120/3 = 50s

Problema: Que pasa si A llega antes que B o C ?

### Shortest time to Completion (STCF)

Se introduce el concepto de _preemtive_ , es decir que un proceso puede ser desalojado del procesador sin que haya terminado de ejecutarse. Es decir, si esta ejecutandose A pero llega B o C que tienen un menor tiempo para completarse, deja de ejecutarse A y pasa a ejecutarse B o C para terminarlos antes.

### Round Robin (RR)

Se ejecuta cada proceso una cantidad fija de tiempo (_slice_) y luego se pasa a otro proceso. Así disminuye el tiempo de respuesta de cada proceso considerablemente. Para esto se utiliza una cola de ejecución. 

Lo **importante** es una buena elección del _time slice_, el cual debe amortiguar el cambio de contexto. Ejemplo: Si el cambio de contexto dura 1 ms y el time slice 10 ms, el cambio de contexto representa un **10%** por lo cual es exagerado. En cambio, si el time slice dura 100 ms, el cambio de contexto representaría un moderado **%1**. 

## Planificación en la vida real

### Multi-level Feedback Queue (MLFQ)

Esta política intenta atacar dos problemas:

1. Optimizar el _turnaround time_, ejecutando la tarea mas corta primero (SJF)
2. Minimizar el  _response time_ utilizando RR. Esto conlleva tener un mal turnaround time.

#### Implementacion

Se tiene una conjunto de distintas colas con un nivel de prioridad asignado.

La tarea en la cola con mayor prioridad sera ejecutada.

##### Reglas básicas

1. Si la prioridad de A > prioridad de B: A se ejecuta por sobre B
2. Si la prioridad de A = prioridad de B: A y B se ejecutan realizando **Round Robin**

Por lo tanto, lo importante esta en como _asignar las prioridades_ y como _variarlas_:

3. Cuando una tarea entra a ejecutarse tiene la prioridad mas alta.
4. Una vez una tarea usa su asignación de tiempo en un nivel dado, su prioridad se reduce (sin importar cuantas veces renuncio a su time slice)

Problema: **Starvation** -> Si hay demasiadas tareas interactivas (dejen de ejecutarse antes de usar el time slice) en el sistema se van a combinar para consumir todo el tiempo del CPU

Solución:

6. Después de cierto periodo de tiempo S, se mueven las tareas a la cola con mas prioridad.

! - Cuanto debería ser S ? -> Es todo un tema (?)

### Proportional Share 

También conocido como _fair-share_, el Proportional Share no busca optimizar el turnaround time ni reducir el time response sino que busca que cada tarea tenga cierto porcentaje de tiempo en el CPU. 

Una implementacion de esta política es _lottery_. Donde cada proceso va a tener mayor probabilidad de ejecutarse según corresponda, pero la determinación de que proceso se ejecuta se realiza mediante un sorteo aleatorio. Esta probabilidad se determina por una cantidad de "boletos" y el sorteo se realiza mediante una simulación. 

#### Mecanismos en la asignación de boletos

##### Ticket Currency

Cada proceso podría tener diferentes "tipos de boletos" con una equivalencia a un valor global (parecido a la cotización de una moneda) y el sistema se encargaría de hacer dicha conversion 

##### Transferencia de boletos

Permite que un proceso transfiera sus boletos a otro de manera temporal. (util para arquitectura cliente-servidor)

##### Inflación

Un proceso puede aumentar o disminuir la cantidad de boletos que posee.

### Planificación Avanzada: Planificación multi procesador


La diferencia sustancia de un procesador mono núcleo con uno multi núcleo, es la utilización de la **cache**.

#### Cache

La cache es una memoria de rápido alcance para el procesador. Para esto se basa en el concepto de _localidad_.

- **Localidad temporal**: Cuando un dato es accedido y es probable que sea utilizado en un futuro muy cercano (variables de un ciclo)

- **Localidad espacial**: Cuando se accede a un dato en una dirección X y es muy probable que se necesite volver a acceder a una dirección cercana a X (valores de un arreglo).


**Bus snooping**: Forma de identificar modificaciones en los valores de una dirección de memoria en relación a el valor guardado en la cache. Se logra observando el bus y detectando actualizaciones de memoria.

**Cache affinity**: Como partes de los procesos se almacenan en la cache de un procesador y esto optimiza el acceso a memoria de los mismos, es interesante que si dicho proceso debe ejecutarse de nuevo, lo haga en el mismo CPU para no tener que hacer de nuevo las lecturas de memoria.

### SQMS (Single Queue Multiprocessor Scheduling)


**Ventajas**: Simplicidad

**Desventajas**: 
- No es escalable 
- Debe insertarse un bloqueo (_lock_) para asegurar que el proceso obtenido de la cola sea correcto
- No se aprovecha la afinidad de la cache

### MQMS (Multi Queue Multiprocessor Scheduling)

Multiples colas con una determinada política de planificación cada una.

(por ejemplo, una cola por procesador)

**Ventajas**: Es escalable, a medida que crecen los CPUs también las colas. También prevee afinidad de cache.

**Desventaja**: 
_load imbalance_ -> Una CPU queda sobrecargada en relación a las otras o viceversa.

Esto se puede resolver con _migration_. Es decir, migrar un proceso de una cola a otra para conseguir balance de carga.

## La vida real

### Linux: Completelly Fair Scheduler (CFS)

Es un _fare share scheduling_ de una forma altamente eficiente y escalable. Intenta tardar poco tiempo en tomar decisiones de planificación.

#### Modo de operación:

Utiliza el concepto de _vitual runtime_ (Vruntime). El cual consta de un runtime normalizado en funcion del numero de procesos runnables. A medida que el proceso se ejecuta gana Vruntime. 

El CFS busca seleccionar el proceso con menor Vruntime para que sea ejecutado.



